{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FastTest_Resnet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMzvVUh8lF+YjIZ8PQXuYl1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DingLi23/NLP_smartdispatch_experiment/blob/main/FastTest_Resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Z-2Rl3MXnv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVGFVOhqMnL5"
      },
      "source": [
        "# !nvidia -smi\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeHcElLTH6fM"
      },
      "source": [
        "# !pip uninstall transformers\n",
        "!pip install transformers==2.2.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0vbhQfPqO23"
      },
      "source": [
        "pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekqY0G5eNOAV"
      },
      "source": [
        "pip install fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTP-B67SNkwa"
      },
      "source": [
        "pip install ltp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YOKnRjceIfA"
      },
      "source": [
        "import csv\n",
        "import re\n",
        "from ltp import LTP\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from gensim.models.word2vec import LineSentence\n",
        "import multiprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyTNlM_Adv2k"
      },
      "source": [
        "**Fasttext**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t2IQ6k14SfA"
      },
      "source": [
        "First, cutwords by LTP\n",
        "\n",
        "Then, train fasttext model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzljZNfVDCDY"
      },
      "source": [
        "input_val = '/content/drive/My Drive/NLP_gov/history_data/val_22_Mar_2021.csv'\n",
        "input_test = '/content/drive/My Drive/NLP_gov/history_data/test_22_Mar_2021.csv'\n",
        "input_train = '/content/drive/My Drive/NLP_gov/history_data/train_22_Mar_2021.csv'\n",
        "save_model_path = '/content/drive/My Drive/NLP_gov/history_data/fasttext.bin'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcck_gnqSfYA"
      },
      "source": [
        "\n",
        "\n",
        "ltp = LTP()\n",
        "\n",
        "#1.read rawdata request sentences\n",
        "\n",
        "def LTP_cutdata(inputfile,col):\n",
        "  rawdata = pd.read_csv(inputfile,usecols = [col])\n",
        "  rawdata = rawdata.values.tolist()\n",
        "  # print(rawdata,type(rawdata))\n",
        "  count = 0\n",
        "  word_list = []\n",
        "  words_list = []\n",
        "  for sentence in rawdata:\n",
        "    # print('point1 sentence:',sentence)\n",
        "    count +=1\n",
        "    if(count == 1):\n",
        "      continue\n",
        "    words, _ = ltp.seg(sentence)\n",
        "    # print('point2 words:',sentence)\n",
        "    words_list.append(words)\n",
        "    for word in words:\n",
        "      word_list.append(word) \n",
        "    if (count % 1000 == 0):\n",
        "      print(count)\n",
        "\n",
        "  # print('point3:gengerate word_list:',word_list)\n",
        "  return(word_list,words_list)\n",
        "\n",
        " \n",
        "#2.Train Fasttext model\n",
        "def train_fasttextmodel(word_list,outputfile):\n",
        "\n",
        "  model = FastText(word_list, size=100, window=5, min_count =1, workers=multiprocessing.cpu_count())\n",
        "  model.save(outputfile)\n",
        "  return(outputfile)\n",
        "\n",
        "word_list_val, words_list_val = LTP_cutdata(input_val,col=1)\n",
        "print('finish val')\n",
        "word_list_test, words_list_test = LTP_cutdata(input_test,col=1)\n",
        "print('finish test')\n",
        "word_list_train, words_list_train = LTP_cutdata(input_train,col=1)\n",
        "print('finish train')\n",
        "\n",
        "word_list = word_list_val + word_list_test + word_list_train\n",
        "\n",
        "train_fasttextmodel(word_list,save_model_path)\n",
        "print('finish all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOoDDSQdWl0c"
      },
      "source": [
        "Save cutwords to txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmoFI00lTkoX"
      },
      "source": [
        "def writelines_txt(word_list,outputfile):\n",
        "  write_file = open(outputfile, 'w', encoding='utf-8')\n",
        "  count = 0\n",
        "  for word in word_list:\n",
        "    count +=1\n",
        "    if(count == 1):\n",
        "      continue\n",
        "    word = str(word)\n",
        "    write_file.write(word)\n",
        "    write_file.write('\\n')\n",
        "    if (count % 10000 == 0):\n",
        "      print(count)\n",
        "  write_file.close()\n",
        "  print('finish')\n",
        "\n",
        "words_list_val_file = '/content/drive/My Drive/NLP_gov/history_data/words_list_val_file.txt'\n",
        "word_list_val_file = '/content/drive/My Drive/NLP_gov/history_data/word_list_val_file.txt'\n",
        "writelines_txt(words_list_val,words_list_val_file)\n",
        "writelines_txt(word_list_val,word_list_val_file)\n",
        "\n",
        "words_list_test_file = '/content/drive/My Drive/NLP_gov/history_data/words_list_test_file.txt'\n",
        "word_list_test_file = '/content/drive/My Drive/NLP_gov/history_data/word_list_test_file.txt'\n",
        "writelines_txt(words_list_test,words_list_test_file)\n",
        "writelines_txt(word_list_test,word_list_test_file)\n",
        "\n",
        "words_list_train_file = '/content/drive/My Drive/NLP_gov/history_data/words_list_train_file.txt'\n",
        "word_list_train_file = '/content/drive/My Drive/NLP_gov/history_data/word_list_train_file.txt'\n",
        "writelines_txt(words_list_train,words_list_train_file)\n",
        "writelines_txt(word_list_train,word_list_train_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm28mMaj4cdE"
      },
      "source": [
        "test fasttext model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ9AM8LF3f_8"
      },
      "source": [
        "model = FastText.load('/content/drive/My Drive/NLP_gov/history_data/fasttext.bin')\n",
        "vocab = model.wv.vocab\n",
        "# print(vocab)\n",
        "print(vocab['年检'].index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWnpIgAH4gBx"
      },
      "source": [
        "Generate csv includes vec_index, class, cutwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7CsIDND1nux"
      },
      "source": [
        "output_train = '/content/drive/My Drive/NLP_gov/history_data/train_out.csv'\n",
        "output_test = '/content/drive/My Drive/NLP_gov/history_data/test_out.csv'\n",
        "output_val = '/content/drive/My Drive/NLP_gov/history_data/val_out.csv'\n",
        "save_model_path = '/content/drive/My Drive/NLP_gov/history_data/fasttext.bin'\n",
        "text_file = '/content/drive/My Drive/NLP_gov/history_data/val.csv'\n",
        "count = 0\n",
        "ltp = LTP()\n",
        "#3.save cut words and classes\n",
        "def get_vocab(Model, path):\n",
        "  model = Model.load(path)\n",
        "  vocab = model.wv.vocab\n",
        "  return vocab\n",
        "\n",
        "def cut_sentence(sentence):\n",
        "\n",
        "  word_list = []\n",
        "  # words_list = []\n",
        "  # print(sentence)\n",
        "  global count\n",
        "  count +=1\n",
        "  if (count % 1000 == 0):\n",
        "    print(count)\n",
        "  sentence = sentence.split(\" \")\n",
        "  # print('list',sentence)\n",
        "  words, _ = ltp.seg(sentence)\n",
        "  # words_list.append(words)\n",
        "  for word in words:\n",
        "    word_list.append(word) \n",
        "  # print('wordslist',words_list)\n",
        "  # print('wordlist',word_list)\n",
        "  return word_list\n",
        "\n",
        "\n",
        "def sentence_to_word_indexes(sentence, vocab):\n",
        "\n",
        "  indexes = []\n",
        "  word_list = cut_sentence(sentence)\n",
        "  # print(word_list)\n",
        "  for word in word_list:\n",
        "    # print('word:::',word)\n",
        "    for char in word:\n",
        "      try:\n",
        "        # print(char)\n",
        "        index = vocab[char].index\n",
        "      except:\n",
        "        continue\n",
        "      indexes.append(index)\n",
        "    # print(indexes)\n",
        "  return str(indexes)\n",
        "\n",
        "\n",
        "def generate_VecIndex(inputfile,savefile): #,cutwords_list):\n",
        "  data = pd.read_csv(inputfile)\n",
        "  new_csv = pd.DataFrame(columns=['orginal sentence','ltp_cutwords', 'class','vec_index'])\n",
        "  new_csv['class'] = data['class']\n",
        "  new_csv['orginal sentence'] = data['sentence_no_symbol']\n",
        "  # new_csv['ltp_cutwords'] = pd.DataFrame(cutwords_list)\n",
        "  print('finish process1')\n",
        "  vocab = get_vocab(FastText,save_model_path)\n",
        "  # print(vocab['公积金'].index)\n",
        "  print('finish vocab')\n",
        "  new_csv['vec_index'] = data['sentence_no_symbol'].apply(sentence_to_word_indexes, args=(vocab,))\n",
        "  print('finish index')\n",
        "  new_csv.to_csv(savefile, index = False)\n",
        "\n",
        "#word_list_val, words_list_val = LTP_cutdata(text_file,col=1)\n",
        "\n",
        "generate_VecIndex(input_val,output_val)#,words_list_val)\n",
        "print('point1')\n",
        "generate_VecIndex(input_test,output_test)#,word_list_test)\n",
        "print('point2')\n",
        "generate_VecIndex(input_train,output_train)#,words_list_train)\n",
        "print('point3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRAKzIVxn5ai"
      },
      "source": [
        "**ResNet Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48AWZkwbn9gU"
      },
      "source": [
        "import json\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec,FastText\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, add, Input, Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import keras_metrics as km\n",
        "from sklearn.metrics import recall_score, precision_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3Sne2OyoJdS"
      },
      "source": [
        "# class Metrics(keras.callbacks.Callback):\n",
        "#     def __init__(self, valid_data):\n",
        "#         super(Metrics, self).__init__()\n",
        "#         self.validation_data = valid_data\n",
        " \n",
        "#     def on_epoch_end(self, epoch, logs=None):\n",
        "#         logs = logs or {}\n",
        "#         val_predict = np.argmax(self.model.predict(self.validation_data[0]), -1)\n",
        "#         val_targ = self.validation_data[1]\n",
        "#         if len(val_targ.shape) == 2 and val_targ.shape[1] != 1:\n",
        "#             val_targ = np.argmax(val_targ, -1)\n",
        " \n",
        "#         _val_f1 = f1_score(val_targ, val_predict, average='macro')\n",
        "#         _val_recall = recall_score(val_targ, val_predict, average='macro')\n",
        "#         _val_precision = precision_score(val_targ, val_predict, average='macro')\n",
        " \n",
        "#         logs['val_f1'] = _val_f1\n",
        "#         logs['val_recall'] = _val_recall\n",
        "#         logs['val_precision'] = _val_precision\n",
        "#         print(\" — val_f1: %f — val_precision: %f — val_recall: %f\" % (_val_f1, _val_precision, _val_recall))\n",
        "\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, dataframe, vectors_of_words, batch_size=128, n_classes=10, shuffle=True):\n",
        "        self.dataframe = dataframe\n",
        "        self.indexes = np.arange(self.dataframe.__len__())\n",
        "        self.n_classes = n_classes\n",
        "        self.labels = to_categorical(self.dataframe['class'], self.n_classes)\n",
        "        self.vectors_of_words = vectors_of_words\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Denotes the number of batches per epoch\n",
        "        \"\"\"\n",
        "        return int(np.floor(self.dataframe.__len__() / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Generate one batch of data\n",
        "        \"\"\"\n",
        "        # Generate indices of the batch\n",
        "        indices = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(indices)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Updates indexes after each epoch\n",
        "        \"\"\"\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, indices):\n",
        "        \"\"\"\n",
        "        Generates data containing batch_size samples\n",
        "        \"\"\"\n",
        "        # Initialization\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        # Generate data\n",
        "        for i, idx in enumerate(indices):\n",
        "            input_indices = json.loads(self.dataframe.iloc[idx, 3])\n",
        "            vec = self.vectors_of_words[input_indices]\n",
        "            if vec.__len__() < 100:\n",
        "                num = 100 - vec.__len__()\n",
        "                vec = np.append(vec, np.zeros((num, 100)), axis=0)\n",
        "            elif vec.__len__() > 100:\n",
        "                vec = vec[:100]\n",
        "            X.append(vec)\n",
        "            y.append(self.labels[idx])\n",
        "        X = np.array(X)\n",
        "        X = np.reshape(X, [X.shape[0], 100, 100, 1])\n",
        "        y = np.array(y)\n",
        "        return X, y\n",
        "\n",
        "\n",
        "def bn_relu(layer, dropout=0, **params):\n",
        "    layer = BatchNormalization()(layer)\n",
        "    layer = Activation(params['conv_activation'])(layer)\n",
        "\n",
        "    if dropout > 0:\n",
        "        layer = Dropout(dropout)(layer)\n",
        "    return layer\n",
        "\n",
        "\n",
        "def res_block(input, filters, stride, dim_up=False):\n",
        "    if dim_up:\n",
        "        shortcut = Conv2D(filters=filters,\n",
        "                          kernel_size=[1, 1],\n",
        "                          use_bias=False)(input)\n",
        "    else:\n",
        "        shortcut = input\n",
        "\n",
        "    layer = Conv2D(filters=filters,\n",
        "                   kernel_size=[1, stride],\n",
        "                   kernel_initializer='random_uniform',\n",
        "                   padding='same')(input)\n",
        "    layer = bn_relu(layer, conv_activation='relu')\n",
        "    layer = Conv2D(filters=filters,\n",
        "                   kernel_size=[1, stride],\n",
        "                   kernel_initializer='random_uniform',\n",
        "                   padding='same')(layer)\n",
        "    layer = add([layer, shortcut])\n",
        "    layer = bn_relu(layer, conv_activation='relu')\n",
        "    return layer\n",
        "\n",
        "\n",
        "def network(num):\n",
        "    input = Input(shape=[100, 100, 1])\n",
        "    layer = Conv2D(filters=64, kernel_size=[1, 100], strides=[1, 100], kernel_initializer='random_uniform')(input)\n",
        "    layer = bn_relu(layer, conv_activation='relu')\n",
        "    layer = res_block(layer, 64, stride=2)\n",
        "    layer = res_block(layer, 128, stride=4, dim_up=True)\n",
        "    layer = res_block(layer, 256, stride=8, dim_up=True)\n",
        "    layer = res_block(layer, 512, stride=16, dim_up=True)\n",
        "    layer = Flatten()(layer)\n",
        "    output = Dense(num, activation='softmax')(layer)\n",
        "    model = Model(inputs=[input], outputs=[output])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "kinds_of_label = 157\n",
        "model = network(kinds_of_label)\n",
        "\n",
        "FastText_models = FastText.load('/content/drive/My Drive/NLP_gov/history_data/fasttext.bin')\n",
        "vectors = FastText_models.wv.vectors\n",
        "\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    return keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=0.1)\n",
        "\n",
        "\n",
        "adam = optimizers.Adam(lr=0.001)\n",
        "model.compile(loss=custom_loss,\n",
        "              optimizer=adam,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "training_batch_size = 64\n",
        "test_batch_size = 1\n",
        "\n",
        "train_set = pd.read_csv(output_train)\n",
        "eval_set = pd.read_csv(output_val)\n",
        "test_set = pd.read_csv(output_test)\n",
        "\n",
        "training_generator = DataGenerator(dataframe=train_set, vectors_of_words=vectors,\n",
        "                                   batch_size=training_batch_size, n_classes=kinds_of_label)\n",
        "validation_generator = DataGenerator(dataframe=eval_set, vectors_of_words=vectors,\n",
        "                                     batch_size=training_batch_size, n_classes=kinds_of_label)\n",
        "test_generator = DataGenerator(dataframe=test_set, vectors_of_words=vectors,\n",
        "                               batch_size=test_batch_size, n_classes=kinds_of_label, shuffle=False)\n",
        "\n",
        "model.fit_generator(generator=training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    epochs=20,\n",
        "                    callbacks=[monitor])\n",
        "\n",
        "score = model.evaluate_generator(generator=test_generator)\n",
        "predictions = model.predict_generator(test_generator)\n",
        "print('test loss:', score[0], '; test accuracy:', score[1])\n",
        "model.save('/content/drive/My Drive/NLP_gov/history_data/RCNN_lr001_withLS01.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOfrafSRAVQx"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import recall_score, precision_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "output_test = '/content/drive/My Drive/NLP_gov/history_data/test_out.csv'\n",
        "testdata = pd.read_csv(output_test)\n",
        "labels = np.array(testdata['class'].values)\n",
        "label_pred = np.argmax(predictions,axis=1)\n",
        "\n",
        "con_mat = confusion_matrix(labels, label_pred)\n",
        "# # ValueError: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets\n",
        "recall = np.diag(con_mat) / np.sum(con_mat, axis = 1)\n",
        "precision = np.diag(con_mat) / np.sum(con_mat, axis = 0)\n",
        "avg_recall = np.mean(recall)\n",
        "avg_precision = np.mean(precision)\n",
        "print('avg_precision:',avg_precision,'\\n','avg_recall:',avg_recall)\n",
        "\n",
        "con_mat_pd = pd.DataFrame(con_mat)\n",
        "con_mat_pd.to_csv('/content/drive/My Drive/NLP_gov/history_data/con_mat.csv', index = True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}